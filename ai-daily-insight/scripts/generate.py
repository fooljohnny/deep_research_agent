"""
generate.py â€” Stage-2 prompt: turn structural analysis + trend signals
into a Markdown insight post.

Takes the five-dimension structural change JSON from Stage-1 and the
trend comparison report, then asks the LLM to produce a Markdown blog
post focused on *structural shifts* and *historical context*.
"""

import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Any
import json

from llm_client import get_client, get_model

logger = logging.getLogger(__name__)

CONTENT_DIR = Path(__file__).resolve().parent.parent / "content"

SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä½AIäº§ä¸šç»“æ„æ´å¯Ÿåšä¸»ï¼Œæ¯å¤©å‘å¸ƒä¸€ç¯‡ç»“æ„æ€§å˜åŒ–åˆ†æã€‚

ä½ å°†æ”¶åˆ°ä¸¤ä»½æ•°æ®ï¼š
1. **ä»Šæ—¥ç»“æ„åˆ†æJSON**ï¼šäº”ä¸ªç»´åº¦ï¼ˆæŠ€æœ¯å±‚ã€åŸºç¡€è®¾æ–½ã€åº”ç”¨å±‚ã€èµ„æœ¬ä¿¡å·ã€é£é™©ä¿¡å·ï¼‰
2. **è¶‹åŠ¿å¯¹æ¯”æŠ¥å‘ŠJSON**ï¼šä»Šæ—¥è¯é¢˜å‘é‡ä¸è¿‡å»30å¤©çš„å¯¹æ¯”ç»“æœ

### å†™ä½œåŸåˆ™
- è¿™ä¸æ˜¯æ–°é—»æ‘˜è¦ï¼Œè€Œæ˜¯ç»“æ„æ€§å˜åŒ–æ´å¯Ÿã€‚
- æ ¸å¿ƒé—®é¢˜æ˜¯ï¼šä»Šå¤©çš„ä¿¡æ¯æ„å‘³ç€AIäº§ä¸šæ ¼å±€å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ï¼Ÿ
- æœ‰å˜åŒ–çš„ç»´åº¦æ·±å…¥å±•å¼€ï¼Œæ— å˜åŒ–çš„ç»´åº¦ä¸€ç¬”å¸¦è¿‡ã€‚
- **å¦‚æœè¶‹åŠ¿æŠ¥å‘Šä¸­æœ‰ä¿¡å·ï¼ŒåŠ¡å¿…åœ¨ç›¸åº”ç»´åº¦ä¸­ä½“ç°å†å²å¯¹æ¯”è§†è§’**ï¼š
  - çªç„¶å¢å¼ºçš„è¯é¢˜ï¼šæŒ‡å‡º"è¿™ä¸ªæ–¹å‘åœ¨è¿‡å»å‡ å¤©çªç„¶å‡æ¸©"
  - æŒç»­å¢å¼ºçš„è¶‹åŠ¿ï¼šæŒ‡å‡º"è¿™ä¸ªè¶‹åŠ¿å·²ç»è¿ç»­Nå¤©å¢å¼º"
  - æ–°å‡ºç°çš„è¯é¢˜ï¼šæŒ‡å‡º"è¿™æ˜¯30å¤©å†…é¦–æ¬¡å‡ºç°çš„å…¨æ–°æ–¹å‘"
- å¦‚æœè¶‹åŠ¿æŠ¥å‘Šæ˜¾ç¤ºå†å²æ•°æ®ä¸è¶³ï¼Œåˆ™è·³è¿‡è¶‹åŠ¿éƒ¨åˆ†ã€‚
- è¯­è¨€è¦çŠ€åˆ©ã€æœ‰è§‚ç‚¹ã€ä¸åºŸè¯ã€‚
- ç”¨ä¸­æ–‡æ’°å†™ã€‚

### æ ¼å¼è¦æ±‚
- ä»¥ YAML front-matter å¼€å¤´ï¼ˆtitle, date, tagsï¼‰ã€‚
- ç¬¬ä¸€æ®µæ˜¯"ä»Šæ—¥æ ¸å¿ƒåˆ¤æ–­"ï¼ˆcore_insightï¼‰ï¼Œç”¨å¼•ç”¨å—æ ¼å¼ï¼ˆ> ï¼‰ã€‚
- å¦‚æœæœ‰è¶‹åŠ¿ä¿¡å·ï¼Œç´§è·Ÿä¸€ä¸ª ## è¶‹åŠ¿é›·è¾¾ sectionï¼Œç”¨ç®€æ´çš„åˆ—è¡¨å±•ç¤ºæ£€æµ‹åˆ°çš„è¶‹åŠ¿ä¿¡å·ã€‚
- ç„¶åæŒ‰äº”ä¸ªç»´åº¦åˆ†èŠ‚ï¼Œä½¿ç”¨ ## æ ‡é¢˜ï¼š
  - æŠ€æœ¯å±‚ï¼šæ¨¡å‹èƒ½åŠ›ä¸æ–°èŒƒå¼
  - åŸºç¡€è®¾æ–½å±‚ï¼šæ¨ç†æˆæœ¬ä¸ç®—åŠ›
  - åº”ç”¨å±‚ï¼šåœºæ™¯æ‰©å±•ä¸æ›¿ä»£æ•ˆåº”
  - èµ„æœ¬ä¿¡å·ï¼šèèµ„ä¸æˆ˜ç•¥
  - é£é™©ä¿¡å·ï¼šç›‘ç®¡ä¸ä¼¦ç†
- æ¯ä¸ªç»´åº¦å¼€å¤´æ ‡æ³¨å˜åŒ–å¼ºåº¦æ ‡ç­¾ï¼š`ğŸ”´ é‡å¤§çªç ´` `ğŸŸ¡ æ¸è¿›æ”¹å–„` `âšª æ— æ˜¾è‘—å˜åŒ–` `ğŸ”´ å¼ºä¿¡å·` `ğŸŸ¡ ä¸­ç­‰ä¿¡å·` `âšª å¼±ä¿¡å·`
- æœ‰å˜åŒ–çš„ç»´åº¦ï¼šåˆ†æå˜åŒ–æœ¬è´¨ + å¼•ç”¨è¯æ®æ–‡ç« ï¼ˆå«é“¾æ¥ï¼‰+ å†å²è¶‹åŠ¿å¯¹æ¯”ï¼ˆå¦‚æœ‰ï¼‰ã€‚
- æ— å˜åŒ–çš„ç»´åº¦ï¼šç®€çŸ­è¯´æ˜å³å¯ï¼ˆ1-2å¥è¯ï¼‰ã€‚
- æœ€åä¸€èŠ‚ ## å€¼å¾—å…³æ³¨çš„é“¾æ¥ï¼šåˆ—å‡ºåˆ†æä¸­å¼•ç”¨çš„å…³é”®æ–‡ç« é“¾æ¥ã€‚
- æ€»å­—æ•°æ§åˆ¶åœ¨ 800-1500 å­—ã€‚
"""


def _build_user_prompt(
    analysis: dict[str, Any],
    trend_report: dict[str, Any] | None = None,
) -> str:
    parts = [
        "## ä»Šæ—¥ç»“æ„åˆ†æ\n",
        json.dumps(analysis, indent=2, ensure_ascii=False),
    ]

    if trend_report and trend_report.get("has_enough_history"):
        parts.append("\n\n## è¶‹åŠ¿å¯¹æ¯”æŠ¥å‘Š\n")
        parts.append(json.dumps(trend_report, indent=2, ensure_ascii=False))
    elif trend_report:
        parts.append(
            f"\n\n## è¶‹åŠ¿å¯¹æ¯”\n{trend_report.get('message', 'å†å²æ•°æ®ä¸è¶³ï¼Œæš‚æ— è¶‹åŠ¿åˆ†æã€‚')}"
        )

    return "\n".join(parts)


def generate_post(
    analysis: dict[str, Any],
    trend_report: dict[str, Any] | None = None,
) -> str:
    """
    Stage-2 generation: analysis + trends â†’ Markdown blog post.

    Returns the Markdown string and writes it to content/<date>.md.
    """
    client = get_client()
    model = get_model()

    user_prompt = _build_user_prompt(analysis, trend_report)
    logger.info("Sending analysis + trends to LLM (%s) for Stage-2 generation â€¦", model)

    response = client.chat.completions.create(
        model=model,
        temperature=0.5,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
    )

    markdown: str = response.choices[0].message.content or ""

    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    CONTENT_DIR.mkdir(parents=True, exist_ok=True)
    out_path = CONTENT_DIR / f"{today}.md"
    out_path.write_text(markdown, encoding="utf-8")
    logger.info("Blog post written to %s (%d chars)", out_path, len(markdown))

    return markdown


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    sample_analysis = {
        "date": "2026-02-26",
        "title": "AgentèŒƒå¼åŠ é€Ÿè½åœ°ï¼Œèµ„æœ¬å¯†é›†æ¶Œå…¥AIåŸºç¡€è®¾æ–½",
        "core_insight": "AI Agentä»æ¦‚å¿µéªŒè¯è¿›å…¥äº§ä¸šçº§éƒ¨ç½²ï¼ŒåŸºç¡€è®¾æ–½å±‚èèµ„å¯†åº¦åˆ›å­£åº¦æ–°é«˜ã€‚",
        "keywords": ["agent", "infrastructure", "RAG", "inference"],
        "dimensions": {
            "technology": {
                "has_change": True,
                "intensity": "æ¸è¿›æ”¹å–„",
                "model_capability": "å¤šä¸ªå¼€æºæ¨¡å‹åœ¨æ¨ç†èƒ½åŠ›ä¸Šé€¼è¿‘é—­æºå‰æ²¿ã€‚",
                "new_paradigm": "Agentç¼–æ’æ¡†æ¶è¶‹äºæ ‡å‡†åŒ–ã€‚",
                "evidence": [
                    {"title": "Sample Paper", "source": "arXiv cs.AI", "url": "https://arxiv.org/example"}
                ],
            },
            "infrastructure": {
                "has_change": True,
                "intensity": "é‡å¤§çªç ´",
                "inference_cost": "æ–°ä¸€ä»£æ¨ç†èŠ¯ç‰‡å°†å•ä½æˆæœ¬é™ä½40%ã€‚",
                "compute_trend": "äº‘å‚å•†å¼€å§‹æä¾›Agentä¸“ç”¨ç®—åŠ›å®ä¾‹ã€‚",
                "evidence": [
                    {"title": "Sample Infra", "source": "TechCrunch AI", "url": "https://techcrunch.com/example"}
                ],
            },
            "application": {
                "has_change": False, "intensity": "æ— æ˜¾è‘—å˜åŒ–",
                "new_industries": "", "displacement": "", "evidence": [],
            },
            "capital": {
                "has_change": True, "intensity": "å¼ºä¿¡å·",
                "funding_trend": "åŸºç¡€è®¾æ–½èµ›é“æœ¬å‘¨ç¬¬ä¸‰ç¬”å¤§é¢èèµ„ã€‚",
                "valuation": "å¤´éƒ¨AIå…¬å¸ä¼°å€¼ç»§ç»­ä¸Šè¡Œã€‚",
                "strategic_moves": "æŸå¤§å‚æ”¶è´­æ¨ç†ä¼˜åŒ–åˆ›ä¸šå…¬å¸ã€‚",
                "evidence": [
                    {"title": "Sample Funding", "source": "Crunchbase News", "url": "https://crunchbase.com/example"}
                ],
            },
            "risk": {
                "has_change": False, "intensity": "å¼±ä¿¡å·",
                "regulation": "", "ethics_safety": "", "supply_chain": "", "evidence": [],
            },
        },
    }
    sample_trends = {
        "has_enough_history": True,
        "history_days": 15,
        "overall_novelty": 0.35,
        "signals": [
            {
                "type": "continuous_trend",
                "dimension": "æŠ€æœ¯å±‚",
                "description": "æŠ€æœ¯å±‚è¯é¢˜å‘é‡åœ¨æœ€è¿‘7å¤©å†…æŒç»­å¢å¼ºã€‚",
                "trend_days": 7,
                "confidence": 0.80,
            }
        ],
        "keyword_trends": {
            "new_keywords": ["agent-orchestration"],
            "rising_keywords": ["inference", "RAG"],
            "fading_keywords": ["diffusion"],
        },
    }
    md = generate_post(sample_analysis, sample_trends)
    print(md)
